{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"austen.txt\", 'r', encoding='utf-8') as f1:\n",
    "    text = f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dataset in characters: 5292297\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of the dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one who had ever seen Catherine Morland in her infancy would have supposed her born to be an heroine. Her situation in life, the character of her father and mother, her own person and disposition, were all equally against her. Her father was a clergyman, without being neglected, or poor, and a very respectable man, though his name was Richard--and he had never been handsome. He had a considerable independence besides two good livings--and he was not in the least addicted to locking up his daughters. Her mother was a woman of useful plain sense, with a good temper, and, what is more remarkable, with a good constitution. She had three sons before Catherine was born; and instead of dying in bringing the latter into the world, as anybody might expect, she still lived on--lived to have six children more--to see them growing up around her, and to enjoy excellent health herself. A family of ten children will be always called a fine family, where there are heads and arms and legs enough for\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters in the text:  \n",
      " !\"&'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz{}£½àáæèéê“”\n",
      "length of the vocabulary:  94\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"unique characters in the text: \", \"\".join(chars))\n",
    "print(\"length of the vocabulary: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 64, 64, 1, 75, 63, 60, 73, 60]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from string to integers\n",
    "stoi = {chars[i]:i for i in range(vocab_size)}\n",
    "itos = {i:chars[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: \"\".join([itos[j] for j in i])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5292297]) torch.int64\n",
      "tensor([39, 70,  1, 70, 69, 60,  1, 78, 63, 70,  1, 63, 56, 59,  1, 60, 77, 60,\n",
      "        73,  1, 74, 60, 60, 69,  1, 28, 56, 75, 63, 60, 73, 64, 69, 60,  1, 38,\n",
      "        70, 73, 67, 56, 69, 59,  1, 64, 69,  1, 63, 60, 73,  1, 64, 69, 61, 56,\n",
      "        69, 58, 80,  1, 78, 70, 76, 67, 59,  1, 63, 56, 77, 60,  1, 74, 76, 71,\n",
      "        71, 70, 74, 60, 59,  1, 63, 60, 73,  1, 57, 70, 73, 69,  1, 75, 70,  1,\n",
      "        57, 60,  1, 56, 69,  1, 63, 60, 73, 70, 64, 69, 60, 11,  1, 33, 60, 73,\n",
      "         1, 74, 64, 75, 76, 56, 75, 64, 70, 69,  1, 64, 69,  1, 67, 64, 61, 60,\n",
      "         9,  1, 75, 63, 60,  1, 58, 63, 56, 73, 56, 58, 75, 60, 73,  1, 70, 61,\n",
      "         1, 63, 60, 73,  1, 61, 56, 75, 63, 60, 73,  1, 56, 69, 59,  1, 68, 70,\n",
      "        75, 63, 60, 73,  9,  1, 63, 60, 73,  1, 70, 78, 69,  1, 71, 60, 73, 74,\n",
      "        70, 69,  1, 56, 69, 59,  1, 59, 64, 74, 71, 70, 74, 64, 75, 64, 70, 69,\n",
      "         9,  1, 78, 60, 73, 60,  1, 56, 67, 67,  1, 60, 72, 76, 56, 67, 67, 80,\n",
      "         1, 56, 62, 56, 64, 69, 74, 75,  1, 63, 60, 73, 11,  1, 33, 60, 73,  1,\n",
      "        61, 56, 75, 63, 60, 73,  1, 78, 56, 74,  1, 56,  1, 58, 67, 60, 73, 62,\n",
      "        80, 68, 56, 69,  9,  1, 78, 64, 75, 63, 70, 76, 75,  1, 57, 60, 64, 69,\n",
      "        62,  1, 69, 60, 62, 67, 60, 58, 75, 60, 59,  9,  1, 70, 73,  1, 71, 70,\n",
      "        70, 73,  9,  1, 56, 69, 59,  1, 56,  1, 77, 60, 73, 80,  1, 73, 60, 74,\n",
      "        71, 60, 58, 75, 56, 57, 67, 60,  1, 68, 56, 69,  9,  1, 75, 63, 70, 76,\n",
      "        62, 63,  1, 63, 64, 74,  1, 69, 56, 68, 60,  1, 78, 56, 74,  1, 43, 64,\n",
      "        58, 63, 56, 73, 59, 10, 10, 56, 69, 59,  1, 63, 60,  1, 63, 56, 59,  1,\n",
      "        69, 60, 77, 60, 73,  1, 57, 60, 60, 69,  1, 63, 56, 69, 59, 74, 70, 68,\n",
      "        60, 11,  1, 33, 60,  1, 63, 56, 59,  1, 56,  1, 58, 70, 69, 74, 64, 59,\n",
      "        60, 73, 56, 57, 67, 60,  1, 64, 69, 59, 60, 71, 60, 69, 59, 60, 69, 58,\n",
      "        60,  1, 57, 60, 74, 64, 59, 60, 74,  1, 75, 78, 70,  1, 62, 70, 70, 59,\n",
      "         1, 67, 64, 77, 64, 69, 62, 74, 10, 10, 56, 69, 59,  1, 63, 60,  1, 78,\n",
      "        56, 74,  1, 69, 70, 75,  1, 64, 69,  1, 75, 63, 60,  1, 67, 60, 56, 74,\n",
      "        75,  1, 56, 59, 59, 64, 58, 75, 60, 59,  1, 75, 70,  1, 67, 70, 58, 66,\n",
      "        64, 69, 62,  1, 76, 71,  1, 63, 64, 74,  1, 59, 56, 76, 62, 63, 75, 60,\n",
      "        73, 74, 11,  1, 33, 60, 73,  1, 68, 70, 75, 63, 60, 73,  1, 78, 56, 74,\n",
      "         1, 56,  1, 78, 70, 68, 56, 69,  1, 70, 61,  1, 76, 74, 60, 61, 76, 67,\n",
      "         1, 71, 67, 56, 64, 69,  1, 74, 60, 69, 74, 60,  9,  1, 78, 64, 75, 63,\n",
      "         1, 56,  1, 62, 70, 70, 59,  1, 75, 60, 68, 71, 60, 73,  9,  1, 56, 69,\n",
      "        59,  9,  1, 78, 63, 56, 75,  1, 64, 74,  1, 68, 70, 73, 60,  1, 73, 60,\n",
      "        68, 56, 73, 66, 56, 57, 67, 60,  9,  1, 78, 64, 75, 63,  1, 56,  1, 62,\n",
      "        70, 70, 59,  1, 58, 70, 69, 74, 75, 64, 75, 76, 75, 64, 70, 69, 11,  1,\n",
      "        44, 63, 60,  1, 63, 56, 59,  1, 75, 63, 73, 60, 60,  1, 74, 70, 69, 74,\n",
      "         1, 57, 60, 61, 70, 73, 60,  1, 28, 56, 75, 63, 60, 73, 64, 69, 60,  1,\n",
      "        78, 56, 74,  1, 57, 70, 73, 69, 24,  1, 56, 69, 59,  1, 64, 69, 74, 75,\n",
      "        60, 56, 59,  1, 70, 61,  1, 59, 80, 64, 69, 62,  1, 64, 69,  1, 57, 73,\n",
      "        64, 69, 62, 64, 69, 62,  1, 75, 63, 60,  1, 67, 56, 75, 75, 60, 73,  1,\n",
      "        64, 69, 75, 70,  1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  9,  1, 56, 74,\n",
      "         1, 56, 69, 80, 57, 70, 59, 80,  1, 68, 64, 62, 63, 75,  1, 60, 79, 71,\n",
      "        60, 58, 75,  9,  1, 74, 63, 60,  1, 74, 75, 64, 67, 67,  1, 67, 64, 77,\n",
      "        60, 59,  1, 70, 69, 10, 10, 67, 64, 77, 60, 59,  1, 75, 70,  1, 63, 56,\n",
      "        77, 60,  1, 74, 64, 79,  1, 58, 63, 64, 67, 59, 73, 60, 69,  1, 68, 70,\n",
      "        73, 60, 10, 10, 75, 70,  1, 74, 60, 60,  1, 75, 63, 60, 68,  1, 62, 73,\n",
      "        70, 78, 64, 69, 62,  1, 76, 71,  1, 56, 73, 70, 76, 69, 59,  1, 63, 60,\n",
      "        73,  9,  1, 56, 69, 59,  1, 75, 70,  1, 60, 69, 65, 70, 80,  1, 60, 79,\n",
      "        58, 60, 67, 67, 60, 69, 75,  1, 63, 60, 56, 67, 75, 63,  1, 63, 60, 73,\n",
      "        74, 60, 67, 61, 11,  1, 26,  1, 61, 56, 68, 64, 67, 80,  1, 70, 61,  1,\n",
      "        75, 60, 69,  1, 58, 63, 64, 67, 59, 73, 60, 69,  1, 78, 64, 67, 67,  1,\n",
      "        57, 60,  1, 56, 67, 78, 56, 80, 74,  1, 58, 56, 67, 67, 60, 59,  1, 56,\n",
      "         1, 61, 64, 69, 60,  1, 61, 56, 68, 64, 67, 80,  9,  1, 78, 63, 60, 73,\n",
      "        60,  1, 75, 63, 60, 73, 60,  1, 56, 73, 60,  1, 63, 60, 56, 59, 74,  1,\n",
      "        56, 69, 59,  1, 56, 73, 68, 74,  1, 56, 69, 59,  1, 67, 60, 62, 74,  1,\n",
      "        60, 69, 70, 76, 62, 63,  1, 61, 70, 73])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire dataset and store it in a tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up data into train and validation sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 70,  1, 70, 69, 60,  1, 78, 63])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8   # the maximum size of the block that is fed to the transformer at once\n",
    "\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([39]) the target: 70\n",
      "when input is tensor([39, 70]) the target: 1\n",
      "when input is tensor([39, 70,  1]) the target: 70\n",
      "when input is tensor([39, 70,  1, 70]) the target: 69\n",
      "when input is tensor([39, 70,  1, 70, 69]) the target: 60\n",
      "when input is tensor([39, 70,  1, 70, 69, 60]) the target: 1\n",
      "when input is tensor([39, 70,  1, 70, 69, 60,  1]) the target: 78\n",
      "when input is tensor([39, 70,  1, 70, 69, 60,  1, 78]) the target: 63\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 57, 60, 64, 69, 62,  1, 75],\n",
      "        [67, 67,  1, 37, 56, 59, 80,  1],\n",
      "        [28, 56, 75, 63, 60, 73, 64, 69],\n",
      "        [56,  1, 58, 67, 60, 73, 66,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 60, 64, 69, 62,  1, 75, 63],\n",
      "        [67,  1, 37, 56, 59, 80,  1, 43],\n",
      "        [56, 75, 63, 60, 73, 64, 69, 60],\n",
      "        [ 1, 58, 67, 60, 73, 66,  1, 64]])\n",
      "-------\n",
      "when input is [1] the target: 57\n",
      "when input is [1, 57] the target: 60\n",
      "when input is [1, 57, 60] the target: 64\n",
      "when input is [1, 57, 60, 64] the target: 69\n",
      "when input is [1, 57, 60, 64, 69] the target: 62\n",
      "when input is [1, 57, 60, 64, 69, 62] the target: 1\n",
      "when input is [1, 57, 60, 64, 69, 62, 1] the target: 75\n",
      "when input is [1, 57, 60, 64, 69, 62, 1, 75] the target: 63\n",
      "when input is [67] the target: 67\n",
      "when input is [67, 67] the target: 1\n",
      "when input is [67, 67, 1] the target: 37\n",
      "when input is [67, 67, 1, 37] the target: 56\n",
      "when input is [67, 67, 1, 37, 56] the target: 59\n",
      "when input is [67, 67, 1, 37, 56, 59] the target: 80\n",
      "when input is [67, 67, 1, 37, 56, 59, 80] the target: 1\n",
      "when input is [67, 67, 1, 37, 56, 59, 80, 1] the target: 43\n",
      "when input is [28] the target: 56\n",
      "when input is [28, 56] the target: 75\n",
      "when input is [28, 56, 75] the target: 63\n",
      "when input is [28, 56, 75, 63] the target: 60\n",
      "when input is [28, 56, 75, 63, 60] the target: 73\n",
      "when input is [28, 56, 75, 63, 60, 73] the target: 64\n",
      "when input is [28, 56, 75, 63, 60, 73, 64] the target: 69\n",
      "when input is [28, 56, 75, 63, 60, 73, 64, 69] the target: 60\n",
      "when input is [56] the target: 1\n",
      "when input is [56, 1] the target: 58\n",
      "when input is [56, 1, 58] the target: 67\n",
      "when input is [56, 1, 58, 67] the target: 60\n",
      "when input is [56, 1, 58, 67, 60] the target: 73\n",
      "when input is [56, 1, 58, 67, 60, 73] the target: 66\n",
      "when input is [56, 1, 58, 67, 60, 73, 66] the target: 1\n",
      "when input is [56, 1, 58, 67, 60, 73, 66, 1] the target: 64\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"-------\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 94])\n",
      "tensor(4.8272, grad_fn=<NllLossBackward>)\n",
      "\n",
      "trNLphJ“s'æXxL/H£}Kry3Fkrg8!sm_3_FuL5H5EfPvSWbuziv8Rrè[1jfær))\n",
      "fQbMDPSWdpYé25TW{/HSejá\n",
      "qm50RA^U1a}Kc\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) Batch, Time, Channel (vocab size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# \"\\n\" is encoded into 0. So, we start by feeding a zero to the generate function\n",
    "print(decode(m.generate(idx=torch.zeros([1,1], dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4198219776153564\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SàLifrth pe rat bjeatthes, rlily sisuthithincur s atod Hveshag \n",
      "Dutonde S, oton ag matipanor mare n;\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros([1,1], dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9080d0d9e876040599225a99b678a70781bb2b706bedc1f7d8bf7d473d9175b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
